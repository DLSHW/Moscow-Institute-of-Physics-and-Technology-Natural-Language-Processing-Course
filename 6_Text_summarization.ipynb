{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kmb8UhIzOnfK"
   },
   "source": [
    "# Text Summarization\n",
    "\n",
    "\n",
    "\n",
    "Улучшим два метода: TextRank и Extractive RNN.\n",
    "\n",
    "Датасет: gazeta.ru\n",
    "\n",
    "Возможно, в датасете находятся пустые данные. Проверим эту гипотезу, и если понадобится, сделаем предобработку датасета.\n",
    "\n",
    "\n",
    "`Ноутбук создан на основе семинара Гусева Ильи на кафедре компьютерной лингвистики МФТИ.`\n",
    "\n",
    "Загрузим датасет и необходимые библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OqkLTkFRfXvA"
   },
   "outputs": [],
   "source": [
    "!wget -q https://www.dropbox.com/s/43l702z5a5i2w8j/gazeta_train.txt\n",
    "!wget -q https://www.dropbox.com/s/k2egt3sug0hb185/gazeta_val.txt\n",
    "!wget -q https://www.dropbox.com/s/3gki5n5djs9w0v6/gazeta_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXS1sdYZCluU"
   },
   "outputs": [],
   "source": [
    "!pip install razdel networkx pymorphy2 nltk rouge==0.3.1 summa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5pZ2UGS2DGjH"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def read_gazeta_records(file_name, shuffle=True, sort_by_date=False):\n",
    "    assert shuffle != sort_by_date\n",
    "    records = []\n",
    "    with open(file_name, \"r\") as r:\n",
    "        for line in r:\n",
    "            records.append(json.loads(line))\n",
    "    if sort_by_date:\n",
    "        records.sort(key=lambda x: x[\"date\"])\n",
    "    if shuffle:\n",
    "        random.shuffle\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GNDp-BunEA91"
   },
   "outputs": [],
   "source": [
    "train_records = read_gazeta_records(\"gazeta_train.txt\")\n",
    "val_records = read_gazeta_records(\"gazeta_val.txt\")\n",
    "test_records = read_gazeta_records(\"gazeta_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-gEkSkQDk0P"
   },
   "source": [
    "В качестве метрик здесь и далее используем BLEU и [ROUGE](https://).<br><br>\n",
    "\n",
    "* **ROUGE-N** – measures unigram, bigram, trigram and higher order n-gram overlap\n",
    "* **ROUGE-L** – measures longest matching sequence of words using LCS. An advantage of using LCS is thatit does not require consecutive matches but in-sequence matches that reflect sentence level wordorder. Since it automatically includes longest in-sequence common n-grams, you don’t need apredefined n-gram length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8fVfdfCyCALH"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "def calc_scores(references, predictions, metric=\"all\"):\n",
    "    print(\"Count:\", len(predictions))\n",
    "    print(\"Ref:\", references[-1])\n",
    "    print(\"Hyp:\", predictions[-1])\n",
    "\n",
    "    if metric in (\"bleu\", \"all\"):\n",
    "        print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n",
    "    if metric in (\"rouge\", \"all\"):\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(predictions, references, avg=True)\n",
    "        print(\"ROUGE: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAo9zv_rPtb6",
    "outputId": "bcad35a9-f7bd-42e8-d388-c3f0bef803d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1000\n",
      "Ref: телеканал «спас» запускает реалити-шоу «остров», участникам которого предстоит месяц жить и работать в нило-столобенской пустыни на озере селигер. организаторы отметили, что это беспрецедентный подобный проект на телевидении. участникам шоу будет, где поработать — в монастыре работают свечной, молочный и столярный цеха, есть коровник, конюшня, пасека.\n",
      "Hyp: православный телеканал «спас», учредителем которого является московская патриархия, запускает реалити-шоу «остров», участникам которого предстоит месяц жить и работать в нило-столобенской пустыни на озере селигер в тверской области.\n",
      "BLEU:  0.19177311186434495\n",
      "ROUGE:  {'rouge-1': {'f': 0.23804097238957525, 'p': 0.22208274285774904, 'r': 0.37762764047433917}, 'rouge-2': {'f': 0.10027796832321115, 'p': 0.09647636782929753, 'r': 0.15833772153385062}, 'rouge-l': {'f': 0.1835646488408507, 'p': 0.2022959168891477, 'r': 0.34937017731940756}}\n"
     ]
    }
   ],
   "source": [
    "import razdel\n",
    "\n",
    "def calc_lead_n_score(records, n=3, lower=True, nrows=1000):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    for i, record in enumerate(records):\n",
    "        if i >= nrows:\n",
    "            break\n",
    "\n",
    "        summary = record[\"summary\"]\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        references.append(summary)\n",
    "\n",
    "        text = record[\"text\"]\n",
    "        text = text if not lower else text.lower()\n",
    "        sentences = [sentence.text for sentence in razdel.sentenize(text)]\n",
    "        prediction = \" \".join(sentences[:n])\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    calc_scores(references, predictions)\n",
    "\n",
    "calc_lead_n_score(test_records, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5wKaXPBuQ9e"
   },
   "source": [
    "# Extractive RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdTrfxycB7cd"
   },
   "source": [
    "## Oracle summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Q7DeHDYFSjX"
   },
   "source": [
    "Для сведения задачи к extractive summarization мы должны выбрать те предложения из оригинального текста, которые наиболее похожи на наше целевое summary по нашим метрикам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sxsc0Orf8hGq"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n",
    "    '''\n",
    "    Жадное построение oracle summary\n",
    "    '''\n",
    "    gold_summary = gold_summary.lower() if lower else gold_summary\n",
    "    # Делим текст на предложения\n",
    "    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
    "    n_sentences = len(sentences)\n",
    "    oracle_summary_sentences = set()\n",
    "    \n",
    "    score = -1.0\n",
    "    summaries = []\n",
    "    for _ in range(n_sentences):\n",
    "        for i in range(n_sentences):\n",
    "            if i in oracle_summary_sentences:\n",
    "                continue\n",
    "            current_summary_sentences = copy.copy(oracle_summary_sentences)\n",
    "            # Добавляем какое-то предложения к уже существующему summary\n",
    "            current_summary_sentences.add(i)\n",
    "            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n",
    "            # Считаем метрики\n",
    "            current_score = calc_score(current_summary, gold_summary)\n",
    "            summaries.append((current_score, current_summary_sentences))\n",
    "        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n",
    "        # Иначе на этом заканчиваем\n",
    "        best_summary_score, best_summary_sentences = max(summaries)\n",
    "        if best_summary_score <= score:\n",
    "            break\n",
    "        oracle_summary_sentences = best_summary_sentences\n",
    "        score = best_summary_score\n",
    "    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n",
    "    return oracle_summary, oracle_summary_sentences\n",
    "\n",
    "def calc_single_score(pred_summary, gold_summary, rouge):\n",
    "    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160,
     "referenced_widgets": [
      "1fc7bb9de68d49d7aacea2e56aeaff15",
      "81fa75ac402548bab4a97cc26bdd05b4",
      "d5e37f0a87e34c90a9bd7b7b69cbeaaf",
      "fef09df79b434cadbd7b89752015c5a7",
      "8d1f3aba89094565b6c8c68df196c40a",
      "b27881fc2b3e438181cdcfd9f0412581",
      "4670e9a8750d4d548f03c8b74575c6dc",
      "b1340eb9270e40c1acf490baa79b07af"
     ]
    },
    "id": "7T_ak-KDB8rp",
    "outputId": "6f6474f1-4ea8-4186-8238-d076e06a6532"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc7bb9de68d49d7aacea2e56aeaff15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1000\n",
      "Ref: телеканал «спас» запускает реалити-шоу «остров», участникам которого предстоит месяц жить и работать в нило-столобенской пустыни на озере селигер. организаторы отметили, что это беспрецедентный подобный проект на телевидении. участникам шоу будет, где поработать — в монастыре работают свечной, молочный и столярный цеха, есть коровник, конюшня, пасека.\n",
      "Hyp: православный телеканал «спас», учредителем которого является московская патриархия, запускает реалити-шоу «остров», участникам которого предстоит месяц жить и работать в нило-столобенской пустыни на озере селигер в тверской области. в комментарии также отмечается, что это беспрецедентный подобный проект на телевидении. стоит отметить, что участникам шоу будет, где поработать — в монастыре работают свечной, молочный и столярный цеха, есть коровник, конюшня, пасека.\n",
      "BLEU:  0.531336150784986\n",
      "ROUGE:  {'rouge-1': {'f': 0.36951810858804146, 'p': 0.4053281117404892, 'r': 0.3661389123393327}, 'rouge-2': {'f': 0.2087846693590912, 'p': 0.23400300931194973, 'r': 0.20594499639015063}, 'rouge-l': {'f': 0.32342889691715343, 'p': 0.3777106006444112, 'r': 0.33982247044123787}}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import razdel\n",
    "\n",
    "def calc_oracle_score(records, nrows=1000, lower=True):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    rouge = Rouge()\n",
    "  \n",
    "    for i, record in tqdm(enumerate(records)):\n",
    "        if i >= nrows:\n",
    "            break\n",
    "\n",
    "        summary = record[\"summary\"]\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        references.append(summary)\n",
    "\n",
    "        text = record[\"text\"]\n",
    "        predicted_summary, _ = build_oracle_summary_greedy(text, summary, \n",
    "                                                           calc_score=lambda x, y: calc_single_score(x, y, rouge))\n",
    "        predictions.append(predicted_summary)\n",
    "\n",
    "    calc_scores(references, predictions)\n",
    "\n",
    "\n",
    "calc_oracle_score(test_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foLYftYTCAkS"
   },
   "source": [
    "## Extractive RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NC8bEa3ZdLi"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYrjp9FtGdST"
   },
   "source": [
    "Теперь пробуем предсказать oracle summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Sfauc4VGgyw"
   },
   "source": [
    "### BPE\n",
    "Для начала сделаем BPE токенизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiIbVSqIbHBL"
   },
   "outputs": [],
   "source": [
    "!pip install youtokentome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-PMlNJnea4Um"
   },
   "outputs": [],
   "source": [
    "import youtokentome as yttm\n",
    "\n",
    "def train_bpe(records, model_path, model_type=\"bpe\", vocab_size=10000, lower=True):\n",
    "    temp_file_name = \"temp.txt\"\n",
    "    with open(temp_file_name, \"w\") as temp:\n",
    "        for record in records:\n",
    "            text, summary = record['text'], record['summary']\n",
    "            if lower:\n",
    "                summary = summary.lower()\n",
    "                text = text.lower()\n",
    "            if not text or not summary:\n",
    "                continue\n",
    "            temp.write(text + \"\\n\")\n",
    "            temp.write(summary + \"\\n\")\n",
    "    yttm.BPE.train(data=temp_file_name, vocab_size=vocab_size, model=model_path)\n",
    "\n",
    "train_bpe(train_records, \"BPE_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qYHwL9W5naIw"
   },
   "outputs": [],
   "source": [
    "bpe_processor = yttm.BPE('BPE_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOkUL_YIGp-S"
   },
   "source": [
    "### Словарь\n",
    "Составим словарь для индексации токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GhQYN1beiVEC"
   },
   "outputs": [],
   "source": [
    "vocabulary = bpe_processor.vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seH13yXuGt03"
   },
   "source": [
    "### Кэш oracle summary\n",
    "Закэшируем oracle summary, чтобы не пересчитывать их каждый раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167,
     "referenced_widgets": [
      "16da312130e3423695954809b57f9131",
      "7697c6f2fe744babb8ba7e6acfd8979f",
      "d7c2001ebce2466fb1cc22dba0d7e53d",
      "1208b1bcaf7e478fb7e1d3211628ed76",
      "94a187af89ef420bb3c22faf29fcb2ec",
      "aabba88514a84b5ea2647c977036d8b4",
      "c5bc5a7145884722bcde5e3424c74909",
      "b2be0d5cc441462892c945ea972b428e",
      "ad921e77650843d5af2e9f6fcee4d15f",
      "44ed9a7e4045427cbc56502b824aecc7",
      "a384ad762535411fa42d4d91112af162",
      "953fac1baf4a4c2598fbb8c139fcf846",
      "63e58046ac9e4d55809c76e601d2a894",
      "59793c382a694c56930b06c8ca8f59e8",
      "8c0f1645638143b187a6ef2943da5277",
      "e64ac40eaece41619f1300ca3efeba22",
      "79caf4eb236641fcb469e30d4f6b64b6",
      "cc0cf2ea0d7b423c93c3236e38e92d4a",
      "4839ad0a3cc040b5acae22d7ee946348",
      "27ea5b4dbfc54d4387afeb2b0b16b91b",
      "76135b0f02b44cbab7a1597eb1a7ddf1",
      "3100eecdcac2499d893b71d81c478ce1",
      "ac88c647115844b28d3ec9bd50769a51",
      "9cc850250072406b93b02ce213e16f5f"
     ]
    },
    "id": "Jdb-39jO-72q",
    "outputId": "cc0264b7-6694-4fb3-ebe2-adf9c0bb38f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16da312130e3423695954809b57f9131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad921e77650843d5af2e9f6fcee4d15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79caf4eb236641fcb469e30d4f6b64b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "import razdel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n",
    "    rouge = Rouge()\n",
    "    for i, record in tqdm(enumerate(records)):\n",
    "        if i >= nrows:\n",
    "            break\n",
    "        text = record[\"text\"]\n",
    "        summary = record[\"summary\"]\n",
    "\n",
    "        summary = summary.lower() if lower else summary\n",
    "        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
    "        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n",
    "                                                                         lower=lower, max_sentences=max_sentences)\n",
    "        record[\"sentences\"] = sentences\n",
    "        record[\"oracle_sentences\"] = list(sentences_indicies)\n",
    "        record[\"oracle_summary\"] = oracle_summary\n",
    "\n",
    "    return records[:nrows]\n",
    "\n",
    "ext_train_records = add_oracle_summary_to_records(train_records, nrows=2048)\n",
    "ext_val_records = add_oracle_summary_to_records(val_records, nrows=256)\n",
    "ext_test_records = add_oracle_summary_to_records(test_records, nrows=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ld7LiSWdrcub",
    "outputId": "e2f16bc1-584f-4f22-d00b-5c68d0e15a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JCmAlJXm8AP"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('/content/drive/My Drive/ext_records_.dat', 'wb') as f_out:\n",
    "  pickle.dump(ext_train_records, f_out)\n",
    "  pickle.dump(ext_val_records, f_out)\n",
    "  pickle.dump(ext_test_records, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jxH6f5VinQWk"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('/content/drive/My Drive/ext_records_full.dat', 'rb') as f_in:\n",
    "  ext_train_records = pickle.load(f_in)\n",
    "  ext_val_records = pickle.load(f_in)\n",
    "  ext_test_records = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlXXc8qUHC5m"
   },
   "source": [
    "### Составление батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YATQKCuqHPo3",
    "outputId": "64200f8b-8078-4c1e-9f9a-7cd5163acdcd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MNyxstTChK3C"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import razdel\n",
    "import torch\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "class BatchIterator():\n",
    "    def __init__(self, records, vocabulary, batch_size, bpe_processor, \n",
    "                 shuffle=True, lower=True, max_sentences=30, \n",
    "                 max_sentence_length=50, device=torch.device('cpu')):\n",
    "        self.records = records\n",
    "        self.num_samples = len(records)\n",
    "        self.batch_size = batch_size\n",
    "        self.bpe_processor = bpe_processor\n",
    "        self.shuffle = shuffle\n",
    "        self.batches_count = int(math.ceil(self.num_samples / batch_size))\n",
    "        self.lower = lower\n",
    "        self.rouge = Rouge()\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_sentences = max_sentences\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.batches_count\n",
    "    \n",
    "    def __iter__(self):\n",
    "        indices = np.arange(self.num_samples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "        for start in range(0, self.num_samples, self.batch_size):\n",
    "            end = min(start + self.batch_size, self.num_samples)\n",
    "            batch_indices = indices[start:end]\n",
    "\n",
    "            batch_inputs = []\n",
    "            batch_outputs = []\n",
    "            max_sentence_length = self.max_sentence_length # 0\n",
    "            max_sentences = self.max_sentences # 0\n",
    "            batch_records = []\n",
    "\n",
    "            for data_ind in batch_indices:\n",
    "                \n",
    "                record = self.records[data_ind]\n",
    "                batch_records.append(record)\n",
    "                text = record[\"text\"]\n",
    "                summary = record[\"summary\"]\n",
    "                summary = summary.lower() if self.lower else summary\n",
    "\n",
    "                if \"sentences\" not in record:\n",
    "                    sentences = [sentence.text.lower() if self.lower else sentence.text for sentence in razdel.sentenize(text)][:self.max_sentences] # self.max_sentences\n",
    "                else:\n",
    "                    sentences = record[\"sentences\"]\n",
    "                max_sentences = max(len(sentences), max_sentences)\n",
    "                \n",
    "                # номера предложений, которые в нашем саммари\n",
    "                if \"oracle_sentences\" not in record:\n",
    "                    calc_score = lambda x, y: calc_single_score(x, y, self.rouge)\n",
    "                    sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=calc_score, lower=self.lower, max_sentences=self.max_sentences)[1] # self.max_sentences\n",
    "                else:\n",
    "                    sentences_indicies = record[\"oracle_sentences\"]\n",
    "                \n",
    "                # inputs - индексы слов в предложении\n",
    "                inputs = [bpe_processor.encode(sentence)[:self.max_sentence_length] for sentence in sentences] # self.max_sentence_length\n",
    "                max_sentence_length = max(max_sentence_length, max([len(tokens) for tokens in inputs]))\n",
    "                \n",
    "                # получение метки класса предложения\n",
    "                outputs = [int(i in sentences_indicies) for i in range(len(sentences))]\n",
    "                batch_inputs.append(inputs)\n",
    "                batch_outputs.append(outputs)\n",
    "\n",
    "            tensor_inputs = torch.zeros((self.batch_size, max_sentences, max_sentence_length), dtype=torch.long, device=self.device)\n",
    "            tensor_outputs = torch.zeros((self.batch_size, max_sentences), dtype=torch.float32, device=self.device)\n",
    "\n",
    "\n",
    "            for i, inputs in enumerate(batch_inputs):\n",
    "                for j, sentence_tokens in enumerate(inputs):\n",
    "                    tensor_inputs[i][j][:len(sentence_tokens)] = torch.LongTensor(sentence_tokens)\n",
    "\n",
    "            for i, outputs in enumerate(batch_outputs):\n",
    "                tensor_outputs[i][:len(outputs)] = torch.LongTensor(outputs)\n",
    "\n",
    "            yield {\n",
    "                'inputs': tensor_inputs, \n",
    "                'outputs': tensor_outputs,\n",
    "                'records': batch_records\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5ug9MIObdi03"
   },
   "outputs": [],
   "source": [
    "train_iterator = BatchIterator(ext_train_records, vocabulary, 32, bpe_processor, device=device)\n",
    "val_iterator = BatchIterator(ext_val_records, vocabulary, 32, bpe_processor, device=device)\n",
    "test_iterator = BatchIterator(ext_test_records, vocabulary, 32, bpe_processor, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPlJMg0_dQM-"
   },
   "source": [
    "## Extractor -  SummaRuNNer\n",
    " https://arxiv.org/pdf/1611.04230.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsY5qXFFdT6_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "def train_model(model, train_iterator, val_iterator, vocabulary, bpe_processor,\n",
    "                epochs_count=1, loss_every_nsteps=16, lr=0.001, device_name=\"cuda\"):\n",
    "    \n",
    "    params_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Trainable params: {}\".format(params_count))\n",
    "\n",
    "    device = torch.device(device_name)\n",
    "    model = model.to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    for epoch in range(epochs_count):\n",
    "        for step, batch in enumerate(train_iterator):\n",
    "\n",
    "            model.train()\n",
    "            logits = model(batch[\"inputs\"]) # Прямой проход\n",
    "            loss = loss_function(logits, batch[\"outputs\"]) # Подсчёт ошибки\n",
    "\n",
    "            optimizer.zero_grad() # Зануление градиентов, чтобы их спокойно менять на следующей итерации\n",
    "            loss.backward() # Подсчёт градиентов dL/dw\n",
    "            optimizer.step() # Градиентный спуск или его модификации (в данном случае Adam)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            if step % loss_every_nsteps == 0 and step != 0:\n",
    "                val_total_loss = 0\n",
    "                val_batch_count = 0\n",
    "\n",
    "                model.eval()\n",
    "                for _, val_batch in enumerate(val_iterator):\n",
    "                    logits = model(val_batch[\"inputs\"]) # Прямой проход\n",
    "                    val_total_loss += loss_function(logits, batch[\"outputs\"]) # Подсчёт ошибки\n",
    "                    val_batch_count += 1\n",
    "\n",
    "                avg_val_loss = val_total_loss/val_batch_count\n",
    "                print(\"Epoch = {}, Avg Train Loss = {:.4f}, Avg val loss = {:.4f}, Time = {:.2f}s\".format(epoch, total_loss / loss_every_nsteps, avg_val_loss, time.time() - start_time))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "        total_loss = 0\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QDWpFr-Cdv41",
    "outputId": "77ef6251-1129-4f60-da82-fec3045fe5a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501M/501M [00:27<00:00, 18.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_transformer_model = SentenceTransformer('distilbert-multilingual-nli-stsb-quora-ranking', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iW7iS76KeEdO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "class SentenceEncoderRNN(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = sentence_transformer_model\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        \n",
    "        sentences_embeddings = self.encoder.encode(inputs.tolist(),  \n",
    "                                                   batch_size=32, \n",
    "                                                   device=device, \n",
    "                                                   is_pretokenized=True, \n",
    "                                                   convert_to_tensor=True).to(device)\n",
    "\n",
    "        return sentences_embeddings\n",
    "\n",
    "class SentenceTaggerRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 sentence_encoder_hidden_size=768, \n",
    "                 hidden_size=256,\n",
    "                 bidirectional=True,\n",
    "                 n_layers=1,\n",
    "                 dropout=0.3):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        assert hidden_size % num_directions == 0\n",
    "        hidden_size = hidden_size // num_directions\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.sentence_encoder = SentenceEncoderRNN()\n",
    "                                                  \n",
    "        self.rnn_layer = nn.LSTM(sentence_encoder_hidden_size, hidden_size, n_layers, dropout=dropout,\n",
    "                           bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n",
    "        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.tanh_layer = nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "\n",
    "        # [batch_size, seq num, seq_len]\n",
    "        batch_size = inputs.size(0)\n",
    "        sentences_count = inputs.size(1)\n",
    "        tokens_count = inputs.size(2)\n",
    "        inputs = inputs.reshape(-1, tokens_count) \n",
    "        # [batch_size * seq num, seq_len]\n",
    "\n",
    "        embedded_sentences = self.sentence_encoder(inputs)\n",
    "        embedded_sentences = embedded_sentences.reshape(batch_size, sentences_count, -1)\n",
    "        # [batch_size, seq num, hidden_size]\n",
    "\n",
    "        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n",
    "        outputs = self.dropout_layer(outputs)\n",
    "        # [batch_size, seq num, hidden_size]\n",
    "\n",
    "        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n",
    "        # [batch_size, hidden_size]\n",
    "\n",
    "        # W * h^T\n",
    "        content = self.content_linear_layer(outputs).squeeze(2) # 1-representation\n",
    "        # [batch_size, seq num]\n",
    "\n",
    "        # h^T * W * d\n",
    "        salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2) # 2-representation\n",
    "\n",
    "        # [batch_size, seq num, hidden_size] * [batch_size, hidden_size, 1] = [batch_size, seq num, 1]\n",
    "        return content + salience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxpL3AtOrctD"
   },
   "source": [
    "$P\\left(y_{j} = 1 \\mid \\mathbf{h}_{j}, \\mathbf{s}_{j}, \\mathbf{d}\\right)=\\sigma\\left(W_{c} \\mathbf{h}_{j} + \\mathbf{h}_{j}^{T} W_{s} \\mathbf{d}\\right)$\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vm3URJ6YPS8S",
    "outputId": "0089c6bf-a82f-4202-ea8e-9f25c3ad5c79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 135785473\n",
      "Epoch = 0, Avg Train Loss = 0.3115, Avg val loss = 0.2150, Time = 40.35s\n",
      "Epoch = 0, Avg Train Loss = 0.2489, Avg val loss = 0.2361, Time = 37.87s\n",
      "Epoch = 0, Avg Train Loss = 0.2391, Avg val loss = 0.2306, Time = 38.18s\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTaggerRNN()\n",
    "train_model(model, train_iterator, val_iterator, vocabulary, bpe_processor, device_name=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUJGf3h175-I"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def punct_detokenize(text):\n",
    "    text = text.strip()\n",
    "    punctuation = \",.!?:;%\"\n",
    "    closing_punctuation = \")]}\"\n",
    "    opening_punctuation = \"([}\"\n",
    "    for ch in punctuation + closing_punctuation:\n",
    "        text = text.replace(\" \" + ch, ch)\n",
    "    for ch in opening_punctuation:\n",
    "        text = text.replace(ch + \" \", ch)\n",
    "    res = [r'\"\\s[^\"]+\\s\"', r\"'\\s[^']+\\s'\"]\n",
    "    for r in res:\n",
    "        for f in re.findall(r, text, re.U):\n",
    "            text = text.replace(f, f[0] + f[2:-2] + f[-1])\n",
    "    text = text.replace(\"' s\", \"'s\").replace(\" 's\", \"'s\")\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def postprocess(ref, hyp, is_multiple_ref=False, detokenize_after=False, tokenize_after=True):\n",
    "    if is_multiple_ref:\n",
    "        reference_sents = ref.split(\" s_s \")\n",
    "        decoded_sents = hyp.split(\"s_s\")\n",
    "        hyp = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in decoded_sents]\n",
    "        ref = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in reference_sents]\n",
    "        hyp = \" \".join(hyp)\n",
    "        ref = \" \".join(ref)\n",
    "    ref = ref.strip()\n",
    "    hyp = hyp.strip()\n",
    "    if detokenize_after:\n",
    "        hyp = punct_detokenize(hyp)\n",
    "        ref = punct_detokenize(ref)\n",
    "    if tokenize_after:\n",
    "        hyp = hyp.replace(\"@@UNKNOWN@@\", \"<unk>\")\n",
    "        hyp = \" \".join([token.text for token in razdel.tokenize(hyp)])\n",
    "        ref = \" \".join([token.text for token in razdel.tokenize(ref)])\n",
    "    return ref, hyp\n",
    "\n",
    "def inference_summarunner(model, iterator, top_k=3):\n",
    "\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    model.eval()\n",
    "    for batch in test_iterator:\n",
    "\n",
    "        logits = model(batch[\"inputs\"])\n",
    "        in_summary = torch.argsort(logits, dim=1)[:, -top_k:]\n",
    "        \n",
    "        for i in range(len(batch['outputs'])):\n",
    "\n",
    "            summary = batch['records'][i]['summary']\n",
    "            summary = summary.lower()\n",
    "            predicted_summary = ' '.join([batch['records'][i]['sentences'][idx] for idx in in_summary[i].sort()[0]])\n",
    "\n",
    "            summary, predicted_summary = postprocess(summary, predicted_summary)\n",
    "   \n",
    "            references.append(summary)\n",
    "            predictions.append(predicted_summary)\n",
    "\n",
    "    calc_scores(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZ96X37bb_PV",
    "outputId": "0f47588b-2d6f-43b1-a00c-a10a7917a01d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 256\n",
      "Ref: зарубежные спецслужбы намеренно ищут уязвимости в российском it-секторе , чтобы проводить масштабные кибератаки , заявил секретарь совбеза николай патрушев . по его словам , основные цели злоумышленников – объекты критической информационной инфраструктуры рф . эти атаки — за год несколько миллионов случаев — создают угрозу национальной безопасности .\n",
      "Hyp: иностранные спецслужбы проводят целенаправленный поиск уязвимостей российского it-сектора , чтобы массированно его атаковать . об этом заявил секретарь совета безопасности рф николай патрушев , передает риа « новости » . « основными целями для оказания вредоносного воздействия остаются объекты критической информационной инфраструктуры россии , что создаст реальные угрозы национальной безопасности » , – подчеркнул он .\n",
      "BLEU:  0.4483358093852308\n",
      "ROUGE:  {'rouge-1': {'f': 0.31733181247536896, 'p': 0.3031341514395033, 'r': 0.353196756345429}, 'rouge-2': {'f': 0.14277322190268435, 'p': 0.134759321871149, 'r': 0.16339726182715175}, 'rouge-l': {'f': 0.2715462303275749, 'p': 0.2739258120398612, 'r': 0.3186041265424389}}\n"
     ]
    }
   ],
   "source": [
    "inference_summarunner(model, test_iterator, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbvETJLr7UqK"
   },
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7iZnGhUTP88"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq razdel   networkx  rouge==0.3.1 \n",
    "!pip install -Uq transformers youtokentome\n",
    "!pip install pymorphy2\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VvsyOEiiSjxG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pymorphy2\n",
    "import razdel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "stgx5fwhFpYY"
   },
   "outputs": [],
   "source": [
    "test_records = pd.DataFrame(ext_test_records)\n",
    "test_records.index = test_records.reset_index().index\n",
    "test_records['sentences'] = test_records.text.apply(lambda text: [sentence.text for sentence in razdel.sentenize(text)])\n",
    "\n",
    "corpus = []\n",
    "for sentences in test_records.sentences:\n",
    "  corpus.extend(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "di_1k-GHGmW4"
   },
   "outputs": [],
   "source": [
    "# длины предложений\n",
    "seq_len = [len([token.text for token in razdel.tokenize(sentence)]) for sentence in corpus] \n",
    "# число предложений в текстах\n",
    "seq_num = test_records.sentences.apply(lambda sentences: len(sentences)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "kZ1XvcTNG_hH"
   },
   "outputs": [],
   "source": [
    "def sort_corpus_by_len(corpus, seq_len):\n",
    "    corpus_seq_len = list(zip(list(zip(corpus, range(len(corpus)))), seq_len))\n",
    "    sorted_by_len_corpus = sorted(corpus_seq_len, key=lambda x: x[1])[::-1]\n",
    "    sorted_sent_idx = [pair_sent_idx for pair_sent_idx, _ in sorted_by_len_corpus]\n",
    "    sorted_sents = [sent for sent, _ in sorted_sent_idx]\n",
    "    sorted_indices = [idx for _, idx in sorted_sent_idx]\n",
    "    return sorted_sents, sorted_indices\n",
    "\n",
    "def get_corpus_order_embeddings(embeddings, sorted_indices):\n",
    "     embedding_order = sorted(list(zip(embeddings, sorted_indices)), key=lambda idx: idx[1])\n",
    "     embeddings = [emb for emb, _ in embedding_order]\n",
    "     return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ge69o3NlGWXk"
   },
   "outputs": [],
   "source": [
    "def batch_gen(corpus, batch_size):\n",
    "    num_batches = len(corpus) // batch_size + int(len(corpus) % batch_size == 0)\n",
    "\n",
    "    for i in range(num_batches+1):\n",
    "      yield corpus[i*batch_size:((i+1)*batch_size if i != num_batches else None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Bx2045iKI00p"
   },
   "outputs": [],
   "source": [
    "def get_docs_embeddings(model, corpus, iterator, batch_size, seq_num, seq_len, device, sorted=False):\n",
    "    \n",
    "    if sorted:\n",
    "      corpus, sorted_indices = sort_corpus_by_len(corpus, seq_len)\n",
    "\n",
    "    embeddings = []\n",
    "    for sent_batch in iterator(corpus, batch_size):\n",
    "        embeddings.extend(model.encode(sent_batch, batch_size=batch_size, device=device))\n",
    "    \n",
    "    if sorted:\n",
    "      embeddings = get_corpus_order_embeddings(embeddings, sorted_indices)\n",
    "\n",
    "\n",
    "    embeddings = [emb.reshape(1, -1) for emb in embeddings]\n",
    "    array_embeddings = np.array(embeddings)\n",
    "\n",
    "    \n",
    "    docs_embeddings = []\n",
    "    start, end = 0, 0\n",
    "    for length in seq_num:\n",
    "        start, end = end, end + length\n",
    "        docs_embeddings.append(list(array_embeddings[start:end, :, :])) \n",
    "\n",
    "    return docs_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uSCUIWmSKhh",
    "outputId": "d8af88a8-10d6-449b-aed1-d0f692ae4578"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501M/501M [00:20<00:00, 24.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model = SentenceTransformer('embeddings_xlm_distilroberta_paraphrase', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "iaqzd8M5JB9Z"
   },
   "outputs": [],
   "source": [
    "embeddings = get_docs_embeddings(model, corpus, batch_gen, batch_size, seq_num, seq_len, device, sorted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Qk5KIJ2Gyfe1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('/content/drive/My Drive/embeddings_xlm_distilroberta_paraphrase.dat', 'wb') as f_out:\n",
    "  pickle.dump(embeddings, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "kP1PKV8Hvzor"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('/content/drive/My Drive/embeddings_xlm_distilroberta_paraphrase.dat', 'rb') as f_in:\n",
    "  embeddings = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "c6IDdN15v2qk"
   },
   "outputs": [],
   "source": [
    "test_records['embeddings'] = pd.Series(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5Fcsdv9KMqNy"
   },
   "outputs": [],
   "source": [
    "def my_sim(embedding1, embedding2, norm=cosine_similarity):\n",
    "\n",
    "    return norm(embedding1 - embedding2)\n",
    "\n",
    "\n",
    "def gen_text_rank_summary(text, embeddings, calc_similarity=my_sim, summary_part=0.1):\n",
    "    '''\n",
    "    Составление summary с помощью TextRank\n",
    "    '''\n",
    "    # Разбиваем текст на предложения\n",
    "    sentences = [sentence.text for sentence in razdel.sentenize(text)] # список предложений в виде строк\n",
    "    n_sentences = len(sentences)\n",
    "\n",
    "    # Для каждой пары предложений считаем близость\n",
    "    pairs = combinations(range(n_sentences), 2)\n",
    "    scores = [(i, j, calc_similarity(embeddings[i], embeddings[j])) for i, j in pairs]\n",
    "\n",
    "    # Строим граф с рёбрами, равными близости между предложениями\n",
    "    g = nx.Graph()\n",
    "    g.add_weighted_edges_from(scores)\n",
    "\n",
    "    # Считаем PageRank\n",
    "    pr = nx.pagerank(g)\n",
    "    result = [(i, pr[i], s) for i, s in enumerate(sentences) if i in pr]\n",
    "    result.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Выбираем топ предложений\n",
    "    n_summary_sentences = max(int(n_sentences * summary_part), 1)\n",
    "    result = result[:n_summary_sentences]\n",
    "\n",
    "    # Восстанавливаем оригинальный их порядок\n",
    "    result.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Восстанавливаем текст выжимки\n",
    "    predicted_summary = \" \".join([sentence for i, proba, sentence in result])\n",
    "\n",
    "    return predicted_summary\n",
    "\n",
    "\n",
    "def calc_text_rank_score(records, calc_similarity=my_sim, summary_part=0.1, nrows=1000):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    for text, summary, embeddings in records[['text', 'summary', 'embeddings']].values[:nrows]:\n",
    "\n",
    "        references.append(summary)\n",
    "        \n",
    "        predicted_summary = gen_text_rank_summary(text, embeddings, calc_similarity, summary_part)\n",
    "        \n",
    "        predictions.append(predicted_summary)\n",
    "\n",
    "    calc_scores(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRDMzAXmMzcm",
    "outputId": "419ca853-0da6-4dc8-8ff2-e6b618727881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1000\n",
      "Ref: Телеканал «Спас» запускает реалити-шоу «Остров», участникам которого предстоит месяц жить и работать в Нило-Столобенской пустыни на озере Селигер. Организаторы отметили, что это беспрецедентный подобный проект на телевидении. Участникам шоу будет, где поработать — в монастыре работают свечной, молочный и столярный цеха, есть коровник, конюшня, пасека.\n",
      "Hyp: Православный телеканал «Спас», учредителем которого является Московская патриархия, запускает реалити-шоу «Остров», участникам которого предстоит месяц жить и работать в Нило-Столобенской пустыни на озере Селигер в Тверской области. «Здесь только Ты и Бог. Проживи месяц в Ниловой пустыни, выполняя послушания, и найди ответы на вопросы, которые давно беспокоят», — так анонсирует телеканал свой проект.\n",
      "BLEU:  0.34150026022819713\n",
      "ROUGE:  {'rouge-1': {'f': 0.14349482123691146, 'p': 0.1564047351989802, 'r': 0.14319448479089308}, 'rouge-2': {'f': 0.041246771555016486, 'p': 0.04463343504430261, 'r': 0.04137008638679155}, 'rouge-l': {'f': 0.1241732768576413, 'p': 0.14344314048216775, 'r': 0.13143160342304397}}\n"
     ]
    }
   ],
   "source": [
    "calc_text_rank_score(test_records, calc_similarity=my_sim, summary_part=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zYuv7bGwfaO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw_summarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1208b1bcaf7e478fb7e1d3211628ed76": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2be0d5cc441462892c945ea972b428e",
      "placeholder": "​",
      "style": "IPY_MODEL_c5bc5a7145884722bcde5e3424c74909",
      "value": " 52400/? [1:52:01&lt;00:00,  7.80it/s]"
     }
    },
    "16da312130e3423695954809b57f9131": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d7c2001ebce2466fb1cc22dba0d7e53d",
       "IPY_MODEL_1208b1bcaf7e478fb7e1d3211628ed76"
      ],
      "layout": "IPY_MODEL_7697c6f2fe744babb8ba7e6acfd8979f"
     }
    },
    "1fc7bb9de68d49d7aacea2e56aeaff15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d5e37f0a87e34c90a9bd7b7b69cbeaaf",
       "IPY_MODEL_fef09df79b434cadbd7b89752015c5a7"
      ],
      "layout": "IPY_MODEL_81fa75ac402548bab4a97cc26bdd05b4"
     }
    },
    "27ea5b4dbfc54d4387afeb2b0b16b91b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cc850250072406b93b02ce213e16f5f",
      "placeholder": "​",
      "style": "IPY_MODEL_ac88c647115844b28d3ec9bd50769a51",
      "value": " 5770/? [16:19&lt;00:00,  5.89it/s]"
     }
    },
    "3100eecdcac2499d893b71d81c478ce1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44ed9a7e4045427cbc56502b824aecc7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4670e9a8750d4d548f03c8b74575c6dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4839ad0a3cc040b5acae22d7ee946348": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3100eecdcac2499d893b71d81c478ce1",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76135b0f02b44cbab7a1597eb1a7ddf1",
      "value": 1
     }
    },
    "59793c382a694c56930b06c8ca8f59e8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63e58046ac9e4d55809c76e601d2a894": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "76135b0f02b44cbab7a1597eb1a7ddf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7697c6f2fe744babb8ba7e6acfd8979f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79caf4eb236641fcb469e30d4f6b64b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4839ad0a3cc040b5acae22d7ee946348",
       "IPY_MODEL_27ea5b4dbfc54d4387afeb2b0b16b91b"
      ],
      "layout": "IPY_MODEL_cc0cf2ea0d7b423c93c3236e38e92d4a"
     }
    },
    "81fa75ac402548bab4a97cc26bdd05b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c0f1645638143b187a6ef2943da5277": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d1f3aba89094565b6c8c68df196c40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "94a187af89ef420bb3c22faf29fcb2ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "953fac1baf4a4c2598fbb8c139fcf846": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e64ac40eaece41619f1300ca3efeba22",
      "placeholder": "​",
      "style": "IPY_MODEL_8c0f1645638143b187a6ef2943da5277",
      "value": " 5265/? [15:21&lt;00:00,  5.71it/s]"
     }
    },
    "9cc850250072406b93b02ce213e16f5f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a384ad762535411fa42d4d91112af162": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59793c382a694c56930b06c8ca8f59e8",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63e58046ac9e4d55809c76e601d2a894",
      "value": 1
     }
    },
    "aabba88514a84b5ea2647c977036d8b4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac88c647115844b28d3ec9bd50769a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad921e77650843d5af2e9f6fcee4d15f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a384ad762535411fa42d4d91112af162",
       "IPY_MODEL_953fac1baf4a4c2598fbb8c139fcf846"
      ],
      "layout": "IPY_MODEL_44ed9a7e4045427cbc56502b824aecc7"
     }
    },
    "b1340eb9270e40c1acf490baa79b07af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b27881fc2b3e438181cdcfd9f0412581": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2be0d5cc441462892c945ea972b428e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5bc5a7145884722bcde5e3424c74909": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc0cf2ea0d7b423c93c3236e38e92d4a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5e37f0a87e34c90a9bd7b7b69cbeaaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b27881fc2b3e438181cdcfd9f0412581",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d1f3aba89094565b6c8c68df196c40a",
      "value": 1
     }
    },
    "d7c2001ebce2466fb1cc22dba0d7e53d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aabba88514a84b5ea2647c977036d8b4",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_94a187af89ef420bb3c22faf29fcb2ec",
      "value": 1
     }
    },
    "e64ac40eaece41619f1300ca3efeba22": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fef09df79b434cadbd7b89752015c5a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1340eb9270e40c1acf490baa79b07af",
      "placeholder": "​",
      "style": "IPY_MODEL_4670e9a8750d4d548f03c8b74575c6dc",
      "value": " 1000/? [02:30&lt;00:00,  6.59it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
